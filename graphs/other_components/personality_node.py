"""
Defines the `respond_to_user` node factory, which produces the final 
user-facing response in the LangGraph multi-agent system.

This node:
- Wraps a `ChatOpenAI` model configured via environment variables 
  (model name, temperature, max tokens, timeout, API key, and 
  optional rate limiting).
- Injects a role/personality prompt (built with `personality_prompt`) 
  combined with the current date for temporal context.
- Retrieves the conversation history (`messages`) from the shared 
  graph state and prepends a system message.
- Invokes the LLM to generate a response and returns it as the 
  last message, tagged with the name `"personality_node"`.

Public API:
    def respond_to_user(
        State,
        rate_limiter: InMemoryRateLimiter,
        user_message: str,
        current_date: str
    ):
        Factory function returning a callable LangGraph node.

        Args:
            State: The graph state schema (not directly used in logic).
            rate_limiter: Controls API usage to avoid exceeding rate limits.
            user_message: The latest user input that informs personality style.
            current_date: A formatted date string injected into the system prompt.

        Returns:
            A node function that LangGraph will call with the injected state.
            The node consumes the state’s `"messages"` history and appends
            a single new message generated by the model.

        Raises:
            ValueError: If `OPENAI_API_KEY` is not found in environment variables.

        Notes:
            - The returned dictionary must contain a `"messages"` list.
            - The response is always attributed to `"personality_node"`.
"""

from typing import Annotated
from langgraph.prebuilt import InjectedState
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage
from langchain_core.rate_limiters import InMemoryRateLimiter
from prompts.personality_prompt import personality_prompt
from utils.logger_utils import setup_logger
from config.env_config import env

logger = setup_logger(__name__)

LLM = env.llm
LLM_TEMPERATURE = env.llm_temperature
MAX_COMPLETION_TOKENS = env.max_completion_tokens
TIMEOUT = env.llm_timeout
OPENAI_API_KEY = env.openai_api_key

def respond_to_user(State, rate_limiter: InMemoryRateLimiter, user_message: str, current_date: str):
    """
    Fábrica de nó: retorna a função que o LangGraph chamará com o state real.
    """
    def _node(state: Annotated[dict, InjectedState]):
        logger.info("Starting personality node to generate final response to the user...")

        model = ChatOpenAI(
            model=LLM,
            temperature=LLM_TEMPERATURE,
            max_completion_tokens=MAX_COMPLETION_TOKENS,
            timeout=TIMEOUT,
            openai_api_key=OPENAI_API_KEY,
            rate_limiter=rate_limiter,
        )

        # Monta o contexto
        sys_msg = SystemMessage(content=current_date + "\n\n" + personality_prompt(user_message))
        history = state.get("messages", []) or []
        messages = [sys_msg] + history

        # Gera resposta
        response = model.invoke(messages)
        response.name = "personality_node"

        # ⚠️ Retornar lista de mensagens
        return {"messages": [response]}

    return _node
